import os
import sys
import pandas as pd
import torch
from torch.utils.data import DataLoader
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    get_scheduler,
)
from torch.optim import AdamW
from datasets import Dataset as HFDataset
from tqdm.auto import tqdm
import evaluate


# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –≤ sys.path (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from utils.helpers import analyze_dataset, plot_class_distribution


# ==================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ====================
MODEL_NAME = "cointegrated/rubert-tiny2"
NUM_LABELS = 2
BATCH_SIZE = 4
NUM_EPOCHS = 3
LEARNING_RATE = 2e-5
FREEZE_BACKBONE = True


def main():
    print("üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–æ–µ–∫—Ç–∞: –°–µ–Ω—Ç–∏–º–µ–Ω—Ç-–∞–Ω–∞–ª–∏–∑")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ (–ø—É—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞)
    df = pd.read_csv('data/sentiment_dataset.csv')
    print("üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ:")
    analyze_dataset(df)
    plot_class_distribution(df)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ Hugging Face Dataset
    dataset = HFDataset.from_pandas(df)
    dataset = dataset.train_test_split(test_size=0.3, seed=42)
    print(f"üìà –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: Train {len(dataset['train'])}, Test {len(dataset['test'])}")

    # ==================== –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø ====================
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            padding="max_length",
            truncation=True,
            max_length=256,
        )

    print("üî§ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
    tokenized_datasets = dataset.map(tokenize_function, batched=True)
    tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
    tokenized_datasets.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

    train_dataloader = DataLoader(tokenized_datasets["train"], shuffle=True, batch_size=BATCH_SIZE)
    eval_dataloader = DataLoader(tokenized_datasets["test"], batch_size=BATCH_SIZE)

    # ==================== –ú–û–î–ï–õ–¨ ====================
    print(f"ü§ñ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å {MODEL_NAME}...")
    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME,
        num_labels=NUM_LABELS,
    )

    if FREEZE_BACKBONE:
        for param in model.base_model.parameters():
            param.requires_grad = False
        print("‚úÖ –¢–µ–ª–æ –º–æ–¥–µ–ª–∏ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–æ, –æ–±—É—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≥–æ–ª–æ–≤–∫–∞!")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    print(f"üì± –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

    # ==================== –û–ü–¢–ò–ú–ò–ó–ê–¢–û–† –ò –ü–õ–ê–ù–ò–†–û–í–©–ò–ö ====================
    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)
    num_training_steps = NUM_EPOCHS * len(train_dataloader)
    lr_scheduler = get_scheduler(
        "linear",
        optimizer=optimizer,
        num_warmup_steps=0,
        num_training_steps=num_training_steps,
    )

    metric = evaluate.load("accuracy")

    # ==================== –û–ë–£–ß–ï–ù–ò–ï ====================
    progress_bar = tqdm(range(num_training_steps))
    model.train()
    print("üéØ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")

    for epoch in range(NUM_EPOCHS):
        total_loss = 0
        for batch in train_dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            loss.backward()

            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()

            total_loss += loss.item()
            progress_bar.update(1)
            progress_bar.set_description(f"Epoch {epoch + 1}, Loss: {loss.item():.4f}")

        avg_loss = total_loss / len(train_dataloader)
        print(f"üìä Epoch {epoch + 1} –∑–∞–≤–µ—Ä—à—ë–Ω. –°—Ä–µ–¥–Ω–∏–π loss: {avg_loss:.4f}")

    # ==================== –û–¶–ï–ù–ö–ê ====================
    print("üß™ –û—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å...")
    model.eval()
    for batch in eval_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        with torch.no_grad():
            outputs = model(**batch)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=-1)
        metric.add_batch(predictions=predictions, references=batch["labels"])

    final_score = metric.compute()
    print(f"üéâ –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {final_score}")

    # ==================== –°–û–•–†–ê–ù–ï–ù–ò–ï ====================
    model_dir = "models/sentiment_model"
    os.makedirs(model_dir, exist_ok=True)
    model.save_pretrained(model_dir)
    tokenizer.save_pretrained(model_dir)
    print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_dir}/")

    # ==================== –ò–ù–§–ï–†–ï–ù–° ====================
    def predict_sentiment(text):
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=256,
        ).to(device)
        with torch.no_grad():
            outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=-1)
        prediction = torch.argmax(probs, dim=-1).item()
        confidence = probs[0][prediction].item()
        sentiment = "POSITIVE" if prediction == 1 else "NEGATIVE"
        return sentiment, confidence

    test_texts = [
        "–≠—Ç–æ –±—ã–ª —Ö–æ—Ä–æ—à–∏–π —Ñ–∏–ª—å–º —Å –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–º —Å—é–∂–µ—Ç–æ–º",
        "–£–∂–∞—Å–Ω–∞—è –∫–∞—Ä—Ç–∏–Ω–∞, –ø–æ–ª–Ω—ã–π –ø—Ä–æ–≤–∞–ª",
    ]

    print("\nüîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö:")
    for text in test_texts:
        sentiment, confidence = predict_sentiment(text)
        print(f"üìù '{text}'")
        print(f"   ‚Üí {sentiment} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%})")


if __name__ == "__main__":
    main()