# üìÅ scripts/project2_spam.py
import torch
from torch.utils.data import DataLoader
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    get_scheduler
)
from torch.optim import AdamW
from datasets import Dataset
from tqdm.auto import tqdm
import evaluate
import pandas as pd
import os
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from utils.helpers import analyze_dataset, plot_class_distribution

# ==================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ====================
MODEL_NAME = "cointegrated/rubert-tiny2"
NUM_LABELS = 2
BATCH_SIZE = 4
NUM_EPOCHS = 3
LEARNING_RATE = 2e-5
FREEZE_BACKBONE = True

def main():
    print("üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–æ–µ–∫—Ç–∞: –°–ø–∞–º-–¥–µ—Ç–µ–∫—Ü–∏—è")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df = pd.read_csv('data/spam_dataset.csv')
    print("üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ:")
    analyze_dataset(df)
    plot_class_distribution(df)
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ
    dataset = Dataset.from_pandas(df)
    dataset = dataset.train_test_split(test_size=0.3, seed=42)
    print(f"üìà –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: Train {len(dataset['train'])}, Test {len(dataset['test'])}")
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            padding="max_length",
            truncation=True,
            max_length=256
        )
    
    print("üî§ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è...")
    tokenized = dataset.map(tokenize_function, batched=True)
    tokenized = tokenized.rename_column("label", "labels")
    tokenized.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
    
    train_dl = DataLoader(tokenized["train"], shuffle=True, batch_size=BATCH_SIZE)
    eval_dl = DataLoader(tokenized["test"], batch_size=BATCH_SIZE)
    
    # –ú–æ–¥–µ–ª—å
    print(f"ü§ñ –ó–∞–≥—Ä—É–∂–∞–µ–º {MODEL_NAME}...")
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)
    
    if FREEZE_BACKBONE:
        for param in model.base_model.parameters():
            param.requires_grad = False
        print("‚úÖ –¢–µ–ª–æ –º–æ–¥–µ–ª–∏ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–æ")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)
    num_steps = NUM_EPOCHS * len(train_dl)
    scheduler = get_scheduler("linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_steps)
    
    # –ú–µ—Ç—Ä–∏–∫–∞
    metric = evaluate.load("accuracy")
    
    # –û–±—É—á–µ–Ω–∏–µ
    print("üéØ –û–±—É—á–µ–Ω–∏–µ...")
    model.train()
    progress = tqdm(range(num_steps))
    
    for epoch in range(NUM_EPOCHS):
        for batch in train_dl:
            batch = {k: v.to(device) for k, v in batch.items()}
            loss = model(**batch).loss
            loss.backward()
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
            progress.update(1)
            progress.set_description(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")
    
    # –û—Ü–µ–Ω–∫–∞
    print("üß™ –û—Ü–µ–Ω–∫–∞...")
    model.eval()
    for batch in eval_dl:
        batch = {k: v.to(device) for k, v in batch.items()}
        with torch.no_grad():
            outputs = model(**batch)
        preds = torch.argmax(outputs.logits, dim=-1)
        metric.add_batch(predictions=preds, references=batch["labels"])
    
    score = metric.compute()
    print(f"üéâ –¢–æ—á–Ω–æ—Å—Ç—å: {score}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    os.makedirs("./models/spam_model", exist_ok=True)
    model.save_pretrained("./models/spam_model")
    tokenizer.save_pretrained("./models/spam_model")
    print("üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ ./models/spam_model/")
    
    # –¢–µ—Å—Ç
    def predict_spam(text):
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=256).to(device)
        with torch.no_grad():
            logits = model(**inputs).logits
        probs = torch.softmax(logits, dim=-1)
        pred = torch.argmax(probs, dim=-1).item()
        conf = probs[0][pred].item()
        return "SPAM" if pred == 1 else "NOT SPAM", conf
    
    test_texts = [
        "–í—ã –≤—ã–∏–≥—Ä–∞–ª–∏ –º–∏–ª–ª–∏–æ–Ω! –ü–µ—Ä–µ–π–¥–∏—Ç–µ –ø–æ —Å—Å—ã–ª–∫–µ!",
        "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞? –í—Å—Ç—Ä–µ—á–∞–µ–º—Å—è —Å–µ–≥–æ–¥–Ω—è?"
    ]
    
    print("\nüîç –¢–µ—Å—Ç:")
    for t in test_texts:
        label, conf = predict_spam(t)
        print(f"üìù '{t}' ‚Üí {label} ({conf:.1%})")

if __name__ == "__main__":
    main()